export const slides = [
  // Title Slide
  {
    title: "LLM მოდელების საფუძვლები",
    subtitle: "მოდული 1: თანამედროვე AI-ის საფუძვლების გაცნობა",
    background: {
      gradient: "linear-gradient(135deg, #0369a1 0%, #075985 100%)"
    },
    notes: "მოგესალმებით LLM ტრენინგის პირველ მოდულში. დღეს ჩვენ შევისწავლით დიდი ენობრივი მოდელების საფუძვლებს, მათ ისტორიას, ტიპებს და როგორ ვიმუშაოთ მათთან ეფექტურად."
  },
  
  // Agenda Slide
  {
    title: "რას განვიხილავთ",
    content: [
      {
        type: "numbered-list",
        items: [
          "AI განვითარების მოკლე ისტორია",
          "LLM მოდელების ტიპები და მათი შესაძლებლობები",
          "წამყვანი მოდელების მიმოხილვა (GPT, Claude, Llama, Gemini)",
          "მოდელების შედარებითი ანალიზი",
          "პრომფტების შექმნის ხელოვნება (prompt engineering)",
          "LLM-ისთვის როლის მინიჭება და ეფექტური კომუნიკაცია",
          "პრაქტიკული სავარჯიშოები და აპლიკაციები"
        ]
      }
    ],
    notes: "დღეს გვაქვს 3-საათიანი ინტენსიური სესია. დავიწყებთ AI-ის ისტორიითა და ევოლუციით, შევისწავლით სხვადასხვა LLM არქიტექტურას, შევადარებთ წამყვან მოდელებს და შემდეგ გადავალთ პრაქტიკულ ტექნიკებზე ამ მოდელებთან სამუშაოდ."
  },
  
  // Section 1 Intro
  {
    title: "თანამედროვე LLM მოდელების მიმოხილვა",
    subtitle: "წესებიდან ნეირონულ ქსელებამდე და ტრანსფორმერებამდე",
    content: [
      "ამ ნაწილში გამოვიკვლევთ AI სისტემების ევოლუციას, რომელმაც დღევანდელ მძლავრ LLM-ებამდე მიგვიყვანა.",
      {
        type: "list",
        items: [
          "გზა წესებზე დაფუძნებული სისტემებიდან სტატისტიკურ სწავლებამდე",
          "როგორ შეცვალა ნეირონულმა ქსელებმა ბუნებრივი ენის დამუშავება",
          "ტრანსფორმერის არქიტექტურის გარღვევა",
          "მასშტაბირების კანონები და ახალი შესაძლებლობების გაჩენა"
        ]
      }
    ],
    background: {
      color: "#f8fafc"
    },
    notes: "ეს ნაწილი ქმნის საფუძველს იმის გასაგებად, თუ როგორ მივედით დღევანდელ LLM ტექნოლოგიამდე. ევოლუცია წესებიდან სტატისტიკამდე, ღრმა სწავლებამდე და ტრანსფორმერებამდე წარმოადგენს პარადიგმების ცვლილებათა სერიას AI-ში."
  },
  
  // History Timeline
  {
    title: "AI განვითარების მოკლე ისტორია",
    content: [
      "მნიშვნელოვანი ეტაპები AI-ის ევოლუციაში, რომელმაც თანამედროვე LLM-ებამდე მიგვიყვანა",
      {
        type: "list",
        items: [
          "1950-1980-იანი: წესებზე დაფუძნებული სისტემები და სიმბოლური AI",
          "1990-2000-იანი: სტატისტიკური მოდელები და ადრეული ნეირონული ქსელები",
          "2010-2015: ღრმა სწავლების რევოლუცია CNN-ებითა და RNN-ებით",
          "2017: ტრანსფორმერის არქიტექტურა (Attention is All You Need)",
          "2018-დღემდე: ტრანსფორმერული მოდელების მასშტაბირება (GPT სერია და სხვ.)"
        ]
      }
    ],
    notes: "AI-ის ისტორია აღნიშნულია ინოვაციის რამდენიმე ტალღითა და 'AI ზამთრის' პერიოდებით. ამჟამინდელი ტალღა, რომელსაც საფუძვლად უდევს ღრმა სწავლება და ტრანსფორმერები, აღმოჩნდა შესამჩნევად მდგრადი და ნაყოფიერი."
  },
  
  // Early AI History
  {
    title: "ადრეული AI კვლევა (1950-1980-იანი)",
    content: [
      "AI-ის ადრეულ ათწლეულებში დომინირებდა სიმბოლური მიდგომები და ექსპერტული სისტემები.",
      {
        type: "list",
        items: [
          "1950: ალან ტიურინგი გვთავაზობს ტიურინგის ტესტს",
          "1956: დართმუთის კონფერენციაზე იქმნება ტერმინი 'ხელოვნური ინტელექტი'",
          "1960-იანი: ELIZA ჩატბოტი იმიტირებს საუბარს პატერნების შესაბამისობის გამოყენებით",
          "1970-იანი: ექსპერტული სისტემები ცდილობენ ადამიანის ცოდნის წესებად კოდირებას",
          "1980-იანი: AI ზამთარი წესებზე დაფუძნებული მიდგომების შეზღუდვების გამო"
        ]
      },
      {
        type: "quote",
        text: "მანქანები შეძლებენ, ოცი წლის განმავლობაში, შეასრულონ ნებისმიერი სამუშაო, რასაც ადამიანი აკეთებს.",
        author: "ჰერბერტ საიმონი, 1965"
      }
    ],
    notes: "ადრეული AI ოპტიმისტური იყო, მაგრამ საბოლოოდ შეზღუდულია წესებზე დაფუძნებული მიდგომითა და გამოთვლითი შეზღუდვებით. ჰერბერტ საიმონის ციტატა აჩვენებს AI-ის ვადების პროგნოზებში ზედმეტი ოპტიმიზმის განმეორებად ნიმუშს."
  },
  
  // Neural Network Revolution
  {
    title: "ნეირონული ქსელების რევოლუცია",
    content: [
      "2010-იანი წლების დასაწყისში, ღრმა სწავლებამ რევოლუცია მოახდინა AI-ის სფეროში.",
      {
        type: "list",
        items: [
          "2012: AlexNet - გარღვევა კომპიუტერულ მხედველობაში CNN-ით",
          "2014: GAN-ები (Generative Adversarial Networks) - იანის განვითარება",
          "2014-2016: RNN და LSTM მოდელები ტექსტისა და აუდიოსთვის",
          "2015-2016: Word2Vec და GloVe ემბედინგები NLP-სთვის",
          "2016: Google-ის Neural Machine Translation სისტემა"
        ]
      }
    ],
    notes: "ღრმა სწავლების რევოლუციის განმავლობაში ჩვენ ვნახეთ უზარმაზარი პროგრესი ხელოვნური ნეირონული ქსელების გამოყენებით, განსაკუთრებით კონვოლუციური და რეკურენტული ქსელებით, რამაც განაპირობა მნიშვნელოვანი გაუმჯობესებები კომპიუტერულ მხედველობასა და ენის დამუშავებაში."
  },
  
  // Transformer Architecture
  {
    title: "ტრანსფორმერის არქიტექტურა",
    subtitle: "AI-ში რევოლუციის მომტანი ინოვაცია",
    content: [
      "2017 წელს Google-ის მკვლევარებმა წარმოადგინეს ნაშრომი 'Attention is All You Need', რომელიც გახდა LLM-ების განვითარების საფუძველი.",
      {
        type: "list",
        items: [
          "ყურადღების (Attention) მექანიზმი: ენის გრძელვადიანი დამოკიდებულებების დამუშავება",
          "პარალელური გამოთვლები: მნიშვნელოვნად აჩქარებს ტრენინგს",
          "თვითყურადღება (Self-attention): მნიშვნელოვნად აუმჯობესებს კონტექსტის გაგებას",
          "ტრანსფორმერის არქიტექტურის ბლოკები: ენკოდერები და დეკოდერები",
          "გამოყენებები: მანქანური თარგმანი, ტექსტის გენერაცია, კლასიფიკაცია"
        ]
      }
    ],
    notes: "ტრანსფორმერის არქიტექტურამ გადაჭრა ენის მოდელირების ორი ძირითადი პრობლემა: გრძელი კონტექსტის დამუშავება და პარალელიზაცია. ეს მიდგომა გახდა თანამედროვე LLM-ების საფუძველი."
  }
];
